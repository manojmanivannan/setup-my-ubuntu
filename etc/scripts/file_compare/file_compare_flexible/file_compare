#!/usr/bin/env python3


"""
This python script is used to compare the generated feed in the form of csv from the EMC/EDC and validates
with the artifact csv.
The script takes 5 arguments

1.  File path of the generated CSV
2.  File path of the artifact to compare
3.  File path of the validation_config.ini
4.  Project name (eg.eva-ecc-emc,dcloud-end2end)
5.  Name of the test suite (eg.thirdparty-feed-validation,end-2-end)
6.  Run Index
7.  Repetition
8.  File path of the ATF main log file
----------------------------
Date    : 18-October-2019
Author  : Manoj Manivannan
----------------------------
"""
import sys, ast
import os, glob
from datetime import datetime
import pandas as pd
import numpy as np
import configparser
import copy
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


OKGREEN = '\033[32m'
WARNING = '\033[33m'
FAIL    = '\033[31m'
ENDC    = '\033[0m'

generated_file  =   sys.argv[1]
artifact_file   =   sys.argv[2]
config_file_path=   sys.argv[3]
project_name    =   sys.argv[4]
test_case_name  =   str(sys.argv[5])
run_index       =   int(sys.argv[6])
test_rep_index  =   int(sys.argv[7])
main_log_file   =   sys.argv[8]

automation_temp_path=os.path.dirname(generated_file)
if not automation_temp_path:
    automation_temp_path='.'
#search_list=glob.glob(generated_regex)
#search_list.sort(key=os.path.getmtime)

def printinfo(msg):
    output = "[{}][{}][rep{}][{}][run{}][verify]: INFO: {}".format(datetime.now().strftime("%H:%M:%S"),project_name,test_rep_index,test_case_name,run_index,msg)
    f = open(main_log_file,"a+");f.write(output+"\n");f.close
    print (OKGREEN+output+ENDC)

def printerr(msg):
    output = "[{}][{}][rep{}][{}][run{}][verify]: ERROR: {}".format(datetime.now().strftime("%H:%M:%S"),project_name,test_rep_index,test_case_name,run_index,msg)
    f = open(main_log_file,"a+");f.write(output+"\n");f.close
    print (FAIL+output+ENDC)
    #sys.exit(1)

def printwarn(msg, display_error=False):
    output = "[{}][{}][rep{}][{}][run{}][verify]: ALERT: {}".format(datetime.now().strftime("%H:%M:%S"),project_name,test_rep_index,test_case_name,run_index,msg)
    f = open(main_log_file,"a+");f.write(output+"\n");f.close
    WARN_COLOR = FAIL if display_error else WARNING
    print (WARN_COLOR+output+ENDC)

def table_printer(list_1,list_2, list_1_heading, list_2_heading):
    """This function prints missing entires given two lists in table form

        list_1 = ['a', 'b', 'c', 'd', 'e', 'f']
        list_2 = ['b', 'c', 'f', 'z']

        list_1       list_2
        ----------------------
        a            --MISSING--
        b            b
        c            c
        d            --MISSING--
        e            --MISSING--
        f            f
        --MISSING--  z
    """
    from collections import OrderedDict

    mapping = OrderedDict()
    for x in list_1:
        mapping[x] = x if x in list_2 else '--MISSING--'

    for x in list_2:
        mapping[x] = x if x in list_1 else '--MISSING--'

    table_format = '{:<50} {:<50}'
    print(table_format.format(list_1_heading, list_2_heading))
    print('-' * 100)

    for k in mapping:
        if k in list_1:
            print(table_format.format(k, mapping[k]))
        else:
            print(table_format.format(mapping[k], k))

def get_included_columns(path):

    if not os.path.isfile(path):
        printerr("Config file for included columns does not exist: {}".format(path))

    printinfo("Using config file from: {}".format(path))
    config_file = configparser.ConfigParser()
    try:
        config_file.read(path)
    except configparser.MissingSectionHeaderError:
        printerr("{} is not a valid configuration file, is it validation_config.ini ?".format(path))

    try:

        included_columns = list(filter(None,(config_file['Fields_tobe_Included']['Included_Columns_analytics']).split(',')))
        summation_columns = list(filter(None,(config_file['Fields_tobe_Included']['Summation_Columns_analytics']).split(',')))
        variance_columns = ast.literal_eval(config_file['Fields_tobe_Included']['Variance_Columns_analytics'])
        absolute_columns = ast.literal_eval(config_file['Fields_tobe_Included']['Absolute_Columns_analytics'])

    except KeyError as e:
        printerr("'{}'' definition does not exist in {}".format(e.args[0],path))

    try:
        scale_factor = ast.literal_eval(config_file['Fields_tobe_Included']['scale_factor'])
    except:
        printinfo("ScaleFactor not defined, assuming scale_factor = 1")
        scale_factor = 1

    return included_columns, variance_columns, absolute_columns, summation_columns, scale_factor

def select_columns_of_intereset(df1,df2,intr_columns):
    """This function gets only the columns mentioned
    in the validation_config.ini file in both the generated and
    artifact file. does not perform any validation
    """
    # Perform validation to check if both the generated and artifact contain the same records.
    # cases when new record added to product, but not validated/updated in artifact will
    # cause test case to fail
    if not len(df1.columns) == len(df2.columns):
        columns_not_present = list(set(df1.columns).symmetric_difference(set(df2.columns)))
        if len(df1.columns) > len(df2.columns):
            columns_not_present_where = 'artifact'
            columns_present_where     = 'generated'
            columns_not_present_msg   = 'Consider validating those column(s) or add header alone in aritfact to ignore'
        else:
            columns_not_present_where = 'generated'
            columns_present_where     = 'artifact'
            columns_not_present_msg   = 'Implementation required in EMC or typo error'

        # columns_not_present_where = 'artifact' if len(df1.columns) > len(df2.columns) else 'generated'
        # columns_present_where = 'artifact' if len(df1.columns) < len(df2.columns) else 'generated'
        printerr("Column(s) {} in {} csv does not exist in {} csv. {}".format([col for col in columns_not_present if "Unnamed" not in col],columns_present_where,columns_not_present_where,columns_not_present_msg))

    try:
        df1_to_return = df1.drop(df1.columns.difference(intr_columns),axis=1)   # drop the columns that we dont need
        df2_to_return = df2.drop(df2.columns.difference(intr_columns),axis=1)   # drop the columns that we dont need
        # Sort the order of columns before return so both frames are identical
        return df1_to_return.reindex(sorted(df1_to_return.columns), axis=1), df2_to_return.reindex(sorted(df2_to_return.columns), axis=1)

    except Exception as e:

        printerr("Dropping columns failed: {}".format(e))

def read_clean_df(path):
    """
    This function reads the csv into pandas dataframe
    and fills NaN with 0

    Return: Pandas dataframe
    """
    if not os.path.isfile(path):
        printerr("CSV file {} does not exist".format(path))

    try:
        df = pd.read_csv(path, na_values='\\N').fillna(0) # filling Nan values with 0's, since using string will hinder comparison later in functions like tolerance comparison

    except Exception as e:
        printerr("Unable to read csv into pandas with exception: {}".format(e))

    return df

def check_frame_structure(gen_df, art_df, relative_variance, absolute_variance,summation):

    """This function checks if the two dataframes are equal in columns, rows
    and the column length.
    It also checks if the columns in the frames are identical to the ones present in
    the validation_config.ini file
    """
    global type_of_comparison

    if not (len(gen_df) == len(art_df)):
        printerr("Generated CSV does not have the same number of rows as the artifact")

    if relative_variance:
        # choose columns with variances
        variance_col_keys = copy.deepcopy(list(variance_columns.keys()))
        validation_col_list = sorted(variance_col_keys)
        type_of_comparison = 'Variance_Columns_analytics'
    elif absolute_variance:
         absolute_col_keys = copy.deepcopy(list(absolute_columns.keys()))
         validation_col_list = sorted(absolute_col_keys)
         type_of_comparison = 'Absolute_Columns_analytics'
    elif summation:
         absolute_col_keys = copy.deepcopy(summation_columns)
         validation_col_list = sorted(summation_columns)
         type_of_comparison = 'Summation_Columns_analytics'
    else:
        # choose columns with absolute values
        included_col_list = copy.deepcopy(included_columns)
        validation_col_list = sorted(included_col_list)
        type_of_comparison = 'Included_Columns_analytics'

    gen_df_col_list = sorted(list(copy.deepcopy(gen_df.columns))) # create copy and sort it to avoid changing the actual list

    # Finds the difference between the columns in the generated (ignoring artifact df since it was already validated) df
    # versus the ones to be compared in validation
    if len(set(validation_col_list).symmetric_difference(set(gen_df_col_list))) >= 1:
#         table_printer(gen_df_col_list, validation_col_list, list_1_heading='Generated', list_2_heading='Variance columns')
        printerr("Column(s) {} in validation_config.ini ({}) does not exist in artifact and generated csv".format(set(validation_col_list).symmetric_difference(set(gen_df_col_list)), type_of_comparison))


    return True

def print_verbose_failure_msg(df):
    df.dropna(axis=1,how="all",inplace=True)
    for column in df.columns:
        for idx,row in df[column].iteritems():
            if pd.notnull(row):
                if type_of_comparison == 'Variance_Columns_analytics':

                    column_name = column.split('_rtol=')[0]                     # Name of the column
                    scale_factor= "(ScaleFactor="+column.split('_SF=')[1]+")"   # Scale factor(SF) is used when the actual value is SF times the expected. Default is 1
                    symbol='\u00B1'                                             # plus or minus
                    tolerance = str(float(column.split('_rtol=')[1].split('_SF')[0])*100)+"%" # Tolerance value specified in the validation_config.ini. But extracted from the column name here
                    expected = row.split(' | ')[0]                              # Expected value
                    actual = row.split(' | ')[1]                                # Actual value
                    row_id = str(idx)                                           # Row num written to csv

                elif type_of_comparison == 'Absolute_Columns_analytics':

                    column_name = column.split('_atol=')[0]
                    scale_factor= "(ScaleFactor="+column.split('_SF=')[1]+")"
                    symbol='\u003C'          # less than
                    tolerance = str(float(column.split('_atol=')[1].split('_SF')[0]))
                    expected = 'any value'
                    actual = row.split(' | ')[1]
                    row_id = str(idx)

                elif type_of_comparison == 'Summation_Columns_analytics':

                    column_name = column
                    scale_factor=''
                    symbol=''
                    tolerance=''
                    expected = row.split(' | ')[0]
                    actual = row.split(' | ')[1]
                    row_id = str(idx)

                elif type_of_comparison == 'Included_Columns_analytics':

                    column_name = column
                    scale_factor=''
                    symbol=''
                    tolerance=''
                    expected = row.split(' | ')[0]
                    actual = row.split(' | ')[1]
                    row_id = str(idx)

                printwarn("Column '{}'{} expected [{}] {} {}, but recieved [{}] at row {}".format(column_name,scale_factor,expected,symbol,tolerance,actual,row_id), display_error=True)

csv_file_name = os.path.splitext(os.path.basename(generated_file))[0]

#csv_artifact_path = automation_temp_path+'/'+csv_file_name+'_FAIL_RUNINDEX_'+str(run_index)+'.csv'
#if os.path.isfile(csv_artifact_path):
#    os.remove(csv_artifact_path)

def write_csv_failure(df, file_extension):

    csv_artifact_path = automation_temp_path+'/'+csv_file_name+'_'+file_extension+'_FAIL_RUNINDEX_'+str(run_index)+'.csv'
    if os.path.isfile(csv_artifact_path):
        os.remove(csv_artifact_path)

    add_tocsv = False
    #     df.columns = pd.MultiIndex.from_product([df.columns, ['A | G']])
    if os.path.isfile(csv_artifact_path):
        add_tocsv = True
        df_old = pd.read_csv(csv_artifact_path)
    
    df = df.dropna(axis=1,how="all")
    print_verbose_failure_msg(df)

    if add_tocsv:
        df_new = pd.concat([df_old,df],axis=1)
        df_new.to_csv(csv_artifact_path,na_rep='PASSED',index=False)
    else:
        df.to_csv(csv_artifact_path,na_rep='PASSED',index=False)
    printwarn('Artfact save to {}'.format(csv_artifact_path))
        

def mask_frame(difference_df, masker_df):
    """
    This function masks all values where the values of the boolean
    dataframe is True
    """
    masker_df['RowNo'] = [str(i+1) for i in range(len(masker_df))] # Prepare mask dataframe for masking with result
    masker_df.set_index('RowNo',inplace=True)
    masked_df = difference_df.mask(masker_df)
    return masked_df

def compare_dataframes(gen_df,art_df,relative_variance=False,absolute_variance=False,summation=False,scale_factor=1):
    """
    This function compares values of each row between the two dataframes
    and logs the difference if it finds any
    gen_df is the dataframe with generated csv feed values
    art_df is the dataframe with artifact csv values

    Return: Dataframe of difference between generated and artifact, or None if no difference found
    """
    def diff_equality(x):
        return np.nan if x[0]*scale_factor == x[1] else '{} | {}'.format(*x)


    def df_identify_difference(df1,df2):
        """
        This function merges the two dataframes,
        but fills the values of both the frames
        separated by '|' if they are different.
        """
        # df1 is the generated csv
        # df2 is the artifact

        df1['RowNo'] = [str(i+1) for i in range(len(df1))]
        df2['RowNo'] = [str(i+1) for i in range(len(df2))]
        df = pd.concat([df1,df2])
        df_concat = pd.concat(
            [df1.set_index('RowNo'), df2.set_index('RowNo')],
            axis='columns',
            keys=['Generated Feed', 'Artifact'],
            join='outer',
            sort=False
            )
        df_all = df_concat.swaplevel(axis=1)[df1.columns[:-1]]
        return df_all.groupby(level=0, axis=1).apply(lambda frame: frame.apply(diff_equality, axis=1))

    if relative_variance:
        # Here we check if columns are complying within a range.
#         printinfo("Checking for match within tolerance")
        mask_dict = {}
        for col in list(variance_columns.keys()):
            lower_limit = (gen_df[col]-(gen_df[col]*variance_columns[col]))*scale_factor
            upper_limit = (gen_df[col]+(gen_df[col]*variance_columns[col]))*scale_factor
            mask_dict[col] = (art_df[col].between(lower_limit, upper_limit, inclusive=True))

        masker = pd.DataFrame(mask_dict)   # Masked frame where True means they are within tolerance, False otherwise

        if masker.all().all():
            # True only if all values in the dataframe are True
            printinfo("Generated CSV and artifact matched within relative tolerances for column(s) {} | {} column(s) passed".format(list(variance_columns.keys()),len(list(variance_columns.keys()))))
            return None
        else:
            # there have been some false (outside tolerance)
            result = df_identify_difference(gen_df,art_df)
            masked_df = mask_frame(result, masker)
            masked_df = masked_df[masker.columns.values[~(masker.all())]]
            printwarn("Generated CSV and artifact did not match within relative tolerances for column(s) {}".format(masked_df.columns.values))
            masked_df.rename(columns={col: col+'_rtol='+str(tol)+'_SF='+str(scale_factor) for col,tol in variance_columns.items()},inplace=True)
            return masked_df

    elif absolute_variance:
        # Here we check if columns are complying within a range.
#         printinfo("Checking for match within tolerance")
        mask_dict = {}

        for col in list(absolute_columns.keys()):
            mask_dict[col] = (art_df[col].between(0, absolute_columns[col], inclusive=True))

        masker = pd.DataFrame(mask_dict)   # Masked frame where True means they are within tolerance, False otherwise

        if masker.all().all():
            # True only if all values in the dataframe are True
            printinfo("Generated CSV and artifact matched within absolute tolerances for column(s) {} | {} column(s) passed".format(list(absolute_columns.keys()),len(list(absolute_columns.keys()))))
            return None
        else:
            # there have been some false (outside tolerance)
            result = df_identify_difference(gen_df,art_df)
            masked_df = mask_frame(result, masker)
            masked_df = masked_df[masker.columns.values[~(masker.all())]]
            printwarn("Generated CSV and artifact did not match within absolute tolerances for column(s) {}. Test failing".format(masked_df.columns.values))
            masked_df.rename(columns={col: col+'_atol='+str(tol)+'_SF='+str(scale_factor) for col,tol in absolute_columns.items()},inplace=True)
            return masked_df

    elif summation:
        # printinfo("checking for summation of columns match")
        # if there are difference, we get an array with index of location where values are different,
        # and if the size is 0, there was no difference
        gen_df = (pd.DataFrame(gen_df.sum()).transpose())
        art_df = (pd.DataFrame(art_df.sum()).transpose())
        masker = (gen_df != art_df)

        if np.where(masker)[0].size == 0:
            printinfo("Generated CSV and artifact matched exactly for summation of column(s) {} | {} column(s) passed".format(summation_columns,len(summation_columns)))
            return None
        else:
            result = df_identify_difference(gen_df,art_df)
            masked_df = mask_frame(result, masker)
            printwarn("Generated CSV and artifact did not match for summation {}. Test failing".format(result.dropna(axis=1,how="all",inplace=False).columns.to_list()))
            return result

    else:
        # printinfo("checking for absolute match")
        # if there are difference, we get an array with index of location where values are different,
        # and if the size is 0, there was no difference
        masker = (gen_df != art_df)

        if np.where(masker)[0].size == 0:
            printinfo("Generated CSV and artifact matched exactly for column(s) {} | {} column(s) passed".format(included_columns,len(included_columns)))
            return None
        else:
            result = df_identify_difference(gen_df,art_df)
            masked_df = mask_frame(result, masker)
            printwarn("Generated CSV and artifact did not match {}. Test failing".format(result.dropna(axis=1,how="all",inplace=False).columns.to_list()))
            return result

def start_compare(df1,df2,relative_variance=False,absolute_variance=False,summation=False,scale_factor=1):

    if relative_variance:
        if not bool(variance_columns):
            printinfo("Nothing to compare for column(s) with relative variance")
            return
        generated_df, artifact_df = select_columns_of_intereset(df1, df2, list(variance_columns.keys())) # selecting columns of interest

    elif absolute_variance:
        if not bool (absolute_columns):
            printinfo("Nothing to compare for column(s) with absolute variance")
            return
        generated_df, artifact_df = select_columns_of_intereset(df1, df2, list(absolute_columns.keys())) # selecting columns of interest

    elif summation:
        if not bool (summation_columns):
            printinfo("Nothing to compare for column(s) with summation check")
            return
        generated_df, artifact_df = select_columns_of_intereset(df1, df2, list(summation_columns)) # selecting columns of interest

    else:
        if not bool(included_columns):
            printinfo("Nothing to compare for column(s) with exact values")
            return
        generated_df, artifact_df = select_columns_of_intereset(df1, df2, included_columns)

    row_column_check = check_frame_structure(generated_df, artifact_df, relative_variance=relative_variance, absolute_variance=absolute_variance, summation=summation)

    if row_column_check:

        result = compare_dataframes(artifact_df,generated_df,relative_variance=relative_variance,absolute_variance=absolute_variance,summation=summation,scale_factor=scale_factor)

        if result is not None:
            if relative_variance:
                file_ext = 'relative'
            elif absolute_variance:
                file_ext = 'absolute'
            elif summation:
                file_ext='summation'
            else:
                file_ext='exact'
            write_csv_failure(df=result, file_extension=file_ext)
            return False
        
        return True

    else:
        printerr("column(s) and/or row(s) of {} dont match with artifact {}".format(generated_file,artifact_file))
        return False


if __name__ == '__main__':

    if os.path.isfile(artifact_file):
        printinfo("Artifact: {}".format(artifact_file))

    if os.path.isfile(generated_file):
        printinfo("Found generated csv: {}".format(generated_file))
    else:
        printerr("Found no generated csv file: {}".format(generated_file))

    # Loads the config file from where Column(s) to include for comparison are set
    # included_Column(s) is a list of column names with exact values to be checked
    # variance_Column(s) is a dictinonary with column names and their corresponding variance value
    included_columns, variance_columns, absolute_columns, summation_columns, scale_factor = get_included_columns(path=config_file_path)

    # FIXME change logic to check if any column name is repeated in other list
    common_columns_variance = list(set(included_columns).intersection(variance_columns.keys()))
    common_columns_absolute = list(set(included_columns).intersection(absolute_columns.keys()))
    common_columns_abs_var  = list(set(variance_columns.keys()).intersection(absolute_columns.keys()))

    if bool(common_columns_variance):
        printerr("One or more columns found in both 'Absolute_Columns_analytics' and 'Variance_Columns_analytics', Check validation_config.ini for {}".format(common_columns_variance))

    if bool(common_columns_absolute):
        printerr("One or more columns found in both 'Absolute_Columns_analytics' and 'Absolute_Columns_analytics', Check validation_config.ini for {}".format(common_columns_absolute))

    if bool(common_columns_abs_var):
        printwarn("One or more columns found in both 'Variance_Columns_analytics' and 'Absolute_Columns_analytics', Please fix it. Check validation_config.ini for {}".format(common_columns_abs_var), display_error=True)

    # Read the csv and perform comparison
    gen_file = read_clean_df(generated_file)
    art_file = read_clean_df(artifact_file)

    test_status = []


    test_status.append(start_compare(gen_file, art_file, relative_variance=False,  absolute_variance=False,    scale_factor=scale_factor))
    test_status.append(start_compare(gen_file, art_file, relative_variance=True,   absolute_variance=False,    scale_factor=scale_factor))
    test_status.append(start_compare(gen_file, art_file, relative_variance=False,  absolute_variance=True,     scale_factor=scale_factor))
    test_status.append(start_compare(gen_file, art_file, relative_variance=False,  absolute_variance=False,    summation=True,     scale_factor=scale_factor))

    if all(test_status):
        printinfo('Artifact comparison success !')
    else:
        printerr('Artifact comparison failed !')

